# -*- coding: utf-8 -*-
"""YouTube Sentiment Analyzer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQPHtenPUNd5LyXcqNHWoa48PkikY8Sl
"""

import os
import re
import googleapiclient.discovery
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# --- CONFIGURATION ---
# Replace with your actual API Key from Google Cloud Console
API_KEY = "AIzaSyBSd-mps4Vy5hOWV0tm8EN-YUnsb3O7Q3I"

# Replace with the Video ID you want to analyze
# (Found in the URL after 'v=', e.g., https://www.youtube.com/watch?v=dQw4w9WgXcQ -> dQw4w9WgXcQ)
VIDEO_ID = "MK83clSv6-k"

# Limit the number of comments to fetch to avoid hitting API quotas too quickly
MAX_COMMENTS = 200

def get_youtube_comments(api_key, video_id, max_results=100):
    """
    Fetches top-level comments from a specific YouTube video using the YouTube Data API.
    """
    try:
        # Disable OAuthlib's HTTPS verification when running locally.
        # DO NOT leave this in production code.
        os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

        api_service_name = "youtube"
        api_version = "v3"

        youtube = googleapiclient.discovery.build(
            api_service_name, api_version, developerKey=api_key)

        comments = []
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            textFormat="plainText",
            maxResults=min(max_results, 100) # API allows max 100 per page
        )

        print(f"Fetching comments for video ID: {video_id}...")

        while request and len(comments) < max_results:
            response = request.execute()

            for item in response['items']:
                # Extract the comment text and author
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
                comments.append({'author': author, 'comment': comment})

            # Check if there is a next page
            if 'nextPageToken' in response and len(comments) < max_results:
                request = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    textFormat="plainText",
                    pageToken=response['nextPageToken'],
                    maxResults=min(max_results - len(comments), 100)
                )
            else:
                break

        print(f"Successfully fetched {len(comments)} comments.")
        return pd.DataFrame(comments)

    except Exception as e:
        print(f"Error fetching comments: {e}")
        return pd.DataFrame()

def clean_text(text):
    """
    Basic text cleaning: removes special characters (except emojis/punctuation useful for VADER)
    and extra whitespace.
    """
    if not isinstance(text, str):
        return ""
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove newlines
    text = text.replace('\n', ' ')
    return text.strip()

def analyze_sentiment(df):
    """
    Performs sentiment analysis using NLTK's VADER.
    Adds 'compound', 'positive', 'negative', 'neutral' scores and a 'sentiment_category' label.
    """
    print("Downloading VADER lexicon (if not present)...")
    nltk.download('vader_lexicon', quiet=True)

    analyzer = SentimentIntensityAnalyzer()

    # Calculate scores
    # VADER returns a dict: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}
    # Compound score ranges from -1 (Most Negative) to +1 (Most Positive)
    df['scores'] = df['comment'].apply(lambda x: analyzer.polarity_scores(clean_text(x)))

    df['compound'] = df['scores'].apply(lambda x: x['compound'])
    df['pos_score'] = df['scores'].apply(lambda x: x['pos'])
    df['neu_score'] = df['scores'].apply(lambda x: x['neu'])
    df['neg_score'] = df['scores'].apply(lambda x: x['neg'])

    # Categorize based on compound score
    def get_sentiment_label(score):
        if score >= 0.05:
            return 'Positive'
        elif score <= -0.05:
            return 'Negative'
        else:
            return 'Neutral'

    df['sentiment'] = df['compound'].apply(get_sentiment_label)
    return df

def visualize_results(df):
    """
    Generates a Bar Chart of sentiment counts and a Word Cloud of the comments.
    """
    if df.empty:
        print("No data to visualize.")
        return

    # 1. Bar Chart
    plt.figure(figsize=(8, 6))
    sentiment_counts = df['sentiment'].value_counts()
    colors = {'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'}

    sentiment_counts.plot(kind='bar', color=[colors.get(x, 'blue') for x in sentiment_counts.index])
    plt.title('Sentiment Analysis of YouTube Comments')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Comments')
    plt.xticks(rotation=0)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Save or Show
    plt.tight_layout()
    print("Displaying Bar Chart...")
    plt.show()

    # 2. Word Cloud
    all_text = " ".join(comment for comment in df.comment)
    wordcloud = WordCloud(width=800, height=400, background_color ='white', min_font_size=10).generate(all_text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title("Word Cloud of Comments")
    print("Displaying Word Cloud...")
    plt.show()

if __name__ == "__main__":
    # 1. Fetch Data
    if API_KEY == "YOUR_API_KEY_HERE":
        print("WARNING: Please replace 'YOUR_API_KEY_HERE' with your actual Google API Key.")
    else:
        df = get_youtube_comments(API_KEY, VIDEO_ID, MAX_COMMENTS)

        if not df.empty:
            # 2. Analyze Sentiment
            df = analyze_sentiment(df)

            # Display first few rows with sentiment
            print("\n--- Analysis Preview ---")
            print(df[['comment', 'sentiment', 'compound']].head())

            # Print Summary
            print("\n--- Sentiment Summary ---")
            print(df['sentiment'].value_counts())

            # 3. Visualize
            visualize_results(df)
        else:
            print("No comments found or error occurred.")